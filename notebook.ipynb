{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ball tracking in a Volleyball environment\n",
    "> Luca Cazzola, Alessandro Lorenzi\n",
    "\n",
    "*Signal, Image & Video - MSc in Artificial Intelligence Systems - University of Trento* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requirements and loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab # this allows you to control figure size\n",
    "import joblib\n",
    "\n",
    "#control image size\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 15.0) # this controls figure size in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Random Forest classifier\n",
    "rf_classifier = joblib.load(\"models/random_forest_model.joblib\")\n",
    "# Load PCA model\n",
    "pca_model = joblib.load('models/pca_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING\n",
    "---\n",
    "\n",
    "Our main objective is to <b>detect the volley ball</b>. <br><br>\n",
    "Since the ball is often moving, we can start by discriminating what's moving and what's not.\n",
    "<br>\n",
    "The good thing is that our <b>camera is fixed</b>, so an easy method would be to apply the frame difference between the current frame and the previous one, but, since the ball is generally moving fast and the camera frame rate isn't so high, this would lead to duplicates.<br>\n",
    "What we propose is to use instead a **background frame** obtained via the median of N random frames of the video. That's much more robust, as long as the camera stands still.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_background_median(video_path, N):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Randomly select N frames\n",
    "    frameIds = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=N)\n",
    "    \n",
    "    # Store selected frames in an array\n",
    "    frames = []\n",
    "    for fid in frameIds:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "        ret, frame = cap.read()\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    # Calculate the median along the time axis\n",
    "    medianFrame = np.median(frames, axis=0).astype(dtype=np.uint8)\n",
    "    \n",
    "    return medianFrame\n",
    "\n",
    "\n",
    "def get_moving_objects(curr_frame, background):\n",
    "    # apply frame difference\n",
    "    diff_map = cv2.absdiff(curr_frame, background)\n",
    "    \n",
    "    # Thresholding\n",
    "    ## 0 : non-moving object\n",
    "    ## 1 : moving object\n",
    "    threshold = np.mean(diff_map) + 10\n",
    "    \n",
    "    _, binary_map = cv2.threshold(diff_map, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Morphological operations + median blurring\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    binary_map = cv2.erode(binary_map, kernel, iterations=1)\n",
    "    binary_map = cv2.dilate(binary_map, kernel, iterations=5)\n",
    "    binary_map = cv2.medianBlur(binary_map, 5)\n",
    "\n",
    "    return binary_map, diff_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION\n",
    "---\n",
    "\n",
    "Now we want to extract the features of the image. <br><br>\n",
    "First, we use the **HOG (Histogram of Oriented Gradients)** feature extraction process that, basically, counts occurrences of gradient orientation in the image's portion. The descriptor is so focused on the structure or the shape of an object. <br>\n",
    "Then we apply **PCA (Principal Principal Component component Analysis)** to the extracted features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_HOG_features(image):\n",
    "    # Convert the image to grayscale\n",
    "    grayscale_image = image\n",
    "    \n",
    "    # Calculate the HOG features\n",
    "    hog = cv2.HOGDescriptor((64,64), (8,8), (4,4), (4,4), 16, 1 )\n",
    "    hog_features = hog.compute(grayscale_image)\n",
    "    \n",
    "    return hog_features\n",
    "\n",
    "\n",
    "def get_features (image) :\n",
    "    # resize the image\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    \n",
    "    # apply PCA on extracted HOG features \n",
    "    hog_features =  pca_model.transform([extract_HOG_features(image)])[0]\n",
    "    \n",
    "    return hog_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGION IDENTIFICATION\n",
    "---\n",
    "\n",
    "Now we use the feature extraction methods represented before to obtain the **bounding boxes** for each frame, extracting the contours. <br>\n",
    "Once you've done that, it's time to search the ball into the returned boxes, so we apply **classification** with a confidence level that we have set to 0.8 (but can be changed). <br><br>\n",
    "\n",
    "At the end, we provide a graphical representation of the boxes extracted (red) and the ones where the ball is detected (green). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(x, y, w, h):\n",
    "    # Box size limits\n",
    "    cond1 = (150 <= (w * h) <= 3000)\n",
    "    # Aspect ratio limits\n",
    "    cond2 = (0.5 <= (w / h) <= 2)\n",
    "    \n",
    "    return cond1 and cond2\n",
    "\n",
    "    \n",
    "def extract_bboxes (binary_map, diff_map, curr_frame_RGB) :\n",
    "    \n",
    "    # Extract contours and wrap them with bounding boxes\n",
    "    contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours if filter_boxes(*cv2.boundingRect(contour))]\n",
    "        \n",
    "    if len(bounding_boxes) > 0 :\n",
    "        # Extract features from detected bounding boxes\n",
    "        features = [get_features(diff_map[y:y+h, x:x+w]) for x, y, w, h in bounding_boxes]\n",
    "\n",
    "        # Run classification\n",
    "        predictions = rf_classifier.predict_proba(features)\n",
    "\n",
    "        # Show all detected boxes\n",
    "        # If confidence(ball) is at least probability_threshold, box is displayed in GREEN\n",
    "        probability_threshold = 0.8\n",
    "    \n",
    "        for i in range(len(predictions)):\n",
    "            _, prob_ball = predictions[i]\n",
    "            x, y, w, h = bounding_boxes[i]\n",
    "\n",
    "            if prob_ball >= probability_threshold:\n",
    "                cv2.rectangle(curr_frame_RGB, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.rectangle(curr_frame_RGB, (x, y), (x + w, y + h), (255, 0, 0), 2)          \n",
    "\n",
    "    return curr_frame_RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIDEO TESTING\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file : it's possible to change the test between these two videos\n",
    "#video_path = 'videos/vid1_cut.mp4'\n",
    "video_path = 'videos/vid3-cut.mp4'\n",
    "\n",
    "# Generate background frame\n",
    "samples_bck_frames = 50\n",
    "background = generate_background_median(video_path, samples_bck_frames)\n",
    "grayMedianBackground = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a new window for displaying the elaborated frames\n",
    "window_name = \"Detections\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.setWindowProperty(window_name, cv2.WND_PROP_VISIBLE, 10)\n",
    "cv2.resizeWindow(window_name, 800, 600)\n",
    "\n",
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    ret, curr_frame = video_capture.read()  # Read the current frame\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray_curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "    curr_frame_RBG = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get image of moving objects\n",
    "    binary_map, diff_map = get_moving_objects(gray_curr_frame, grayMedianBackground)\n",
    "\n",
    "    # Get bounding boxes of moving objects\n",
    "    detections = extract_bboxes(binary_map, diff_map, curr_frame_RBG)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    detections = cv2.cvtColor(detections, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow(window_name, detections)\n",
    "    \n",
    "    # Check for 'q' or the 'Esc' key\n",
    "    if cv2.waitKey(1) == ord('q') or cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    " \n",
    "# Release the video capture object\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
